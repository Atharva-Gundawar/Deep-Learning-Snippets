{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of better_nlp_summarisers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFyUOrS1fczL"
      },
      "source": [
        "# Better NLP\n",
        "\n",
        "This is a wrapper program/library that encapsulates a couple of NLP libraries that are popular among the AI and ML communities.\n",
        "\n",
        "Examples have been used to illustrate the usage as much as possible. Not all the APIs of the underlying libraries have been covered.\n",
        "\n",
        "The idea is to keep the API language as high-level as possible, so its easier to use and stays human-readable.\n",
        "\n",
        "Libraries / frameworks covered:\n",
        "\n",
        "- nltk [site](http://www.nltk.org/) | [docs](https://buildmedia.readthedocs.org/media/pdf/nltk/latest/nltk.pdf)\n",
        "- numpy [site](https://www.numpy.org/) | [docs](https://docs.scipy.org/doc/)\n",
        "- networkx [site](https://networkx.github.io/) | [docs](https://networkx.github.io/documentation/stable/index.html)\n",
        "\n",
        "See [https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp](https://github.com/neomatrix369/awesome-ai-ml-dl/blob/master/examples/better-nlp) for more details.\n",
        "\n",
        "### This notebook will demonstrate the below NLP features / functionalities, using the above mentioned libraries\n",
        "\n",
        "- Cosine similarity summarisation technique (extractive summarisation)\n",
        "- Vertex ranking algorithm summarisation technique\n",
        "- Build a simple text summarisation tool using NLTK\n",
        "- Summarisation 4 (TODO)\n",
        "- Summarisation 5 (TODO)\n",
        "\n",
        "_Summarisation can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMTAIhz53w8G"
      },
      "source": [
        "### Resources\n",
        "\n",
        "- [Understand Text Summarization and create your own summarizer in python](https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70) or [An Introduction to Text Summarization using the TextRank Algorithm (with Python implementation)](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)\n",
        "- [Beyond bag of words: Using PyTextRank to find Phrases and Summarize text](https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5)\n",
        "- [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f)\n",
        "- [Summarise Text with TFIDF in Python 1/2](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2/2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
        "- [How to Make a Text Summarizer - Intro to Deep Learning #10 by Siraj Raval](https://www.youtube.com/watch?v=ogrJaOIuBx4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lre8GErufczN"
      },
      "source": [
        "#### Setup and installation ( optional )\n",
        "\n",
        "In case, this notebook is running in a local environment (Linux/MacOS) or _Google Colab_ environment and in case it does not have the necessary dependencies installed then please execute the steps in the next section.\n",
        "\n",
        "Otherwise, please SKIP to the **Install Spacy model ( NOT optional )** section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJuCUOMOfczO",
        "outputId": "67c709df-2c59-4ab7-b1df-34e32f40cf98"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "\n",
        "apt-get install apt-utils dselect dpkg\n",
        "\n",
        "echo \"OSTYPE=$OSTYPE\"\n",
        "if [[ \"$OSTYPE\" == \"cygwin\" ]] || [[ \"$OSTYPE\" == \"msys\" ]] ; then\n",
        "    echo \"Windows or Windows-like environment detected, script not tested, and may not work.\"\n",
        "    echo \"Try installing the components mention in the install-[ostype].sh scripts manually.\"\n",
        "    echo \"Or try running under CGYWIN or git-bash.\"\n",
        "    echo \"If successfully installed, please contribute back with the solution via a pull request, to https://github.com/neomatrix369/awesome-ai-ml-dl/\"\n",
        "    echo \"Please give the file a good name, i.e. install-windows.sh or install-windows.bat depending on what kind of script you end up writing\"\n",
        "    exit 0\n",
        "elif [[ \"$OSTYPE\" == \"linux-gnu\" ]] || [[ \"$OSTYPE\" == \"linux\" ]]; then\n",
        "    TARGET_OS=\"linux\"\n",
        "else\n",
        "    TARGET_OS=\"macos\"\n",
        "fi\n",
        "\n",
        "if [[ -e ../../library/org/neomatrix369 ]]; then\n",
        "  echo \"Library source found\"\n",
        "  \n",
        "  cd ../../build\n",
        "  \n",
        "  echo \"Detected OS: ${TARGET_OS}\"\n",
        "  ./install-${TARGET_OS}.sh || true\n",
        "else\n",
        "  if [[ -e awesome-ai-ml-dl/examples/better-nlp/library ]]; then\n",
        "     echo \"Library source found\"\n",
        "  else\n",
        "     git clone \"https://github.com/neomatrix369/awesome-ai-ml-dl\"\n",
        "  fi\n",
        "\n",
        "  echo \"Library source exists\"\n",
        "  cd awesome-ai-ml-dl/examples/better-nlp/build\n",
        "\n",
        "  echo \"Detected OS: ${TARGET_OS}\"\n",
        "  ./install-${TARGET_OS}.sh || true \n",
        "fi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "apt-utils is already the newest version (1.6.13).\n",
            "dpkg is already the newest version (1.19.0.5ubuntu2.3).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  dselect\n",
            "0 upgraded, 1 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 148 kB of archives.\n",
            "After this operation, 1,667 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 dselect amd64 1.19.0.5ubuntu2.3 [148 kB]\n",
            "Fetched 148 kB in 1s (114 kB/s)\n",
            "Selecting previously unselected package dselect.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 160706 files and directories currently installed.)\r\n",
            "Preparing to unpack .../dselect_1.19.0.5ubuntu2.3_amd64.deb ...\r\n",
            "Unpacking dselect (1.19.0.5ubuntu2.3) ...\r\n",
            "Setting up dselect (1.19.0.5ubuntu2.3) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "OSTYPE=linux-gnu\n",
            "Library source exists\n",
            "Detected OS: linux\n",
            "Please check if you fulfill the requirements mentioned in the README file.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [60.9 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [796 kB]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,412 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,183 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,768 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,152 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [423 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,583 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [452 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [904 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.5 kB]\n",
            "Fetched 13.1 MB in 7s (1,817 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "liblapack-dev is already the newest version (3.7.1-4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "curl is already the newest version (7.58.0-2ubuntu3.13).\n",
            "wget is already the newest version (1.19.4-1ubuntu2.2).\n",
            "libswscale-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libswscale-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "zip is already the newest version (3.0-11build1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libgpm2 vim-common vim-runtime xxd\n",
            "Suggested packages:\n",
            "  gpm ctags vim-doc vim-scripts\n",
            "The following NEW packages will be installed:\n",
            "  libgpm2 vim vim-common vim-runtime xxd\n",
            "0 upgraded, 5 newly installed, 0 to remove and 74 not upgraded.\n",
            "Need to get 6,722 kB of archives.\n",
            "After this operation, 32.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xxd amd64 2:8.0.1453-1ubuntu1.4 [49.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-common all 2:8.0.1453-1ubuntu1.4 [70.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgpm2 amd64 1.20.7-5 [15.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim-runtime all 2:8.0.1453-1ubuntu1.4 [5,435 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 vim amd64 2:8.0.1453-1ubuntu1.4 [1,152 kB]\n",
            "Fetched 6,722 kB in 3s (2,321 kB/s)\n",
            "Selecting previously unselected package xxd.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 160753 files and directories currently installed.)\r\n",
            "Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.4_amd64.deb ...\r\n",
            "Unpacking xxd (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Selecting previously unselected package vim-common.\r\n",
            "Preparing to unpack .../vim-common_2%3a8.0.1453-1ubuntu1.4_all.deb ...\r\n",
            "Unpacking vim-common (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Selecting previously unselected package libgpm2:amd64.\r\n",
            "Preparing to unpack .../libgpm2_1.20.7-5_amd64.deb ...\r\n",
            "Unpacking libgpm2:amd64 (1.20.7-5) ...\r\n",
            "Selecting previously unselected package vim-runtime.\r\n",
            "Preparing to unpack .../vim-runtime_2%3a8.0.1453-1ubuntu1.4_all.deb ...\r\n",
            "Adding 'diversion of /usr/share/vim/vim80/doc/help.txt to /usr/share/vim/vim80/doc/help.txt.vim-tiny by vim-runtime'\r\n",
            "Adding 'diversion of /usr/share/vim/vim80/doc/tags to /usr/share/vim/vim80/doc/tags.vim-tiny by vim-runtime'\r\n",
            "Unpacking vim-runtime (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Selecting previously unselected package vim.\r\n",
            "Preparing to unpack .../vim_2%3a8.0.1453-1ubuntu1.4_amd64.deb ...\r\n",
            "Unpacking vim (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Setting up xxd (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Setting up libgpm2:amd64 (1.20.7-5) ...\r\n",
            "Setting up vim-common (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Setting up vim-runtime (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "Setting up vim (2:8.0.1453-1ubuntu1.4) ...\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vim (vim) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vimdiff (vimdiff) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rvim (rvim) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/rview (rview) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/vi (vi) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/view (view) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/ex (ex) in auto mode\r\n",
            "update-alternatives: using /usr/bin/vim.basic to provide /usr/bin/editor (editor) in auto mode\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\r\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "\n",
            "================================================================================\n",
            "================================================================================\n",
            "\n",
            "                              DEPRECATION WARNING                            \n",
            "\n",
            "  Node.js 8.x LTS Carbon is no longer actively supported!\n",
            "\n",
            "  You will not receive security or critical stability updates for this version.\n",
            "\n",
            "  You should migrate to a supported version of Node.js as soon as possible.\n",
            "  Use the installation script that corresponds to the version of Node.js you\n",
            "  wish to install. e.g.\n",
            "\n",
            "   * https://deb.nodesource.com/setup_12.x — Node.js 12 LTS \"Erbium\"\n",
            "   * https://deb.nodesource.com/setup_14.x — Node.js 14 LTS \"Fermium\" (recommended)\n",
            "   * https://deb.nodesource.com/setup_15.x — Node.js 15 \"Fifteen\"\n",
            "   * https://deb.nodesource.com/setup_16.x — Node.js 16 \"Gallium\"\n",
            "\n",
            "  Please see https://github.com/nodejs/Release for details about which\n",
            "  version may be appropriate for you.\n",
            "\n",
            "  The NodeSource Node.js distributions repository contains\n",
            "  information both about supported versions of Node.js and supported Linux\n",
            "  distributions. To learn more about usage, see the repository:\n",
            "    https://github.com/nodesource/distributions\n",
            "\n",
            "================================================================================\n",
            "================================================================================\n",
            "\n",
            "Continuing in 20 seconds ...\n",
            "\n",
            "\n",
            "## Installing the NodeSource Node.js 8.x LTS Carbon repo...\n",
            "\n",
            "\n",
            "## Populating apt-get cache...\n",
            "\n",
            "+ apt-get update\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists...\n",
            "\n",
            "## Confirming \"bionic\" is supported...\n",
            "\n",
            "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_8.x/dists/bionic/Release'\n",
            "\n",
            "## Adding the NodeSource signing key to your keyring...\n",
            "\n",
            "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n",
            "\n",
            "## Creating apt sources list file for the NodeSource Node.js 8.x LTS Carbon repo...\n",
            "\n",
            "+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_8.x bionic main' > /etc/apt/sources.list.d/nodesource.list\n",
            "+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_8.x bionic main' >> /etc/apt/sources.list.d/nodesource.list\n",
            "\n",
            "## Running `apt-get update` for you...\n",
            "\n",
            "+ apt-get update\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Get:2 https://deb.nodesource.com/node_8.x bionic InRelease [4,595 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Get:13 https://deb.nodesource.com/node_8.x bionic/main amd64 Packages [767 B]\n",
            "Hit:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 5,362 B in 2s (2,189 B/s)\n",
            "Reading package lists...\n",
            "\n",
            "## Run `sudo apt-get install -y nodejs` to install Node.js 8.x LTS Carbon and npm\n",
            "## You may also need development tools to build native addons:\n",
            "     sudo apt-get install gcc g++ make\n",
            "## To install the Yarn package manager, run:\n",
            "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n",
            "     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
            "     sudo apt-get update && sudo apt-get install yarn\n",
            "\n",
            "\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  nodejs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 74 not upgraded.\n",
            "Need to get 14.1 MB of archives.\n",
            "After this operation, 70.5 MB of additional disk space will be used.\n",
            "Get:1 https://deb.nodesource.com/node_8.x bionic/main amd64 nodejs amd64 8.17.0-1nodesource1 [14.1 MB]\n",
            "Fetched 14.1 MB in 0s (50.6 MB/s)\n",
            "Selecting previously unselected package nodejs.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 162596 files and directories currently installed.)\r\n",
            "Preparing to unpack .../nodejs_8.17.0-1nodesource1_amd64.deb ...\r\n",
            "Unpacking nodejs (8.17.0-1nodesource1) ...\r\n",
            "Setting up nodejs (8.17.0-1nodesource1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Package libnode64 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'awesome-ai-ml-dl'...\n",
            "E: Package 'libnode64' has no installation candidate\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 381 ms, sys: 48.9 ms, total: 430 ms\n",
            "Wall time: 1min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPLdwvt63w8R"
      },
      "source": [
        "#### Install Spacy model ( NOT optional )\n",
        "\n",
        "Install the large English language model for spaCy - will be needed for the examples in this notebooks.\n",
        "\n",
        "**Note:** from observation it appears that spaCy model should be installed towards the end of the installation process, it avoid errors when running programs using the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dJrJ54a3w8S"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "\n",
        "python -m spacy download en_core_web_lg\n",
        "python -m spacy link en_core_web_lg en || true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwXgEdM8oeUv"
      },
      "source": [
        "## Examples of various summarisation methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX1pZlKofczb"
      },
      "source": [
        "### 1. Cosine similarity summarisation technique (extractive summarisation)\n",
        "\n",
        "**Abstractive Summarization:** Abstractive methods select words based on semantic understanding, even those words did not appear in the source documents. It aims at producing important material in a new way. They interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text.\n",
        "\n",
        "**Flow:** Input document → understand context → semantics → create own summary\n",
        "\n",
        "**Extractive Summarization:** Extractive methods attempt to summarize articles by selecting a subset of words that retain the most important points.\n",
        "\n",
        "**Flow:** Input document → sentences similarity → weight sentences → select sentences with higher rank\n",
        "\n",
        "**Cosine similarity** is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. Angle will be 0 if sentences are similar and tend towards 90 as they begin to differ.\n",
        "\n",
        "Inspired by Praveen Dubey the author of https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70\n",
        "\n",
        "or see [Understand Text Summarization and create your own summarizer in python](https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6zNjWhGpCCt"
      },
      "source": [
        "!pip install pytextrank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjan7P5_3w8Z"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../../library')\n",
        "sys.path.insert(0, './awesome-ai-ml-dl/examples/better-nlp/library')\n",
        "\n",
        "from org.neomatrix369.better_nlp import BetterNLP\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpjkLRHe3w8c"
      },
      "source": [
        "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object\n",
        "generic_text=\"\"\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vkYIGs13w8f"
      },
      "source": [
        "summarised_result = betterNLP.summarise(generic_text)\n",
        "\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
        "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXWSpN7XhQnd"
      },
      "source": [
        "print(\"ranked_sentences=\") \n",
        "pp.pprint(summarised_result['ranked_sentences'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7A3p05vfcz0"
      },
      "source": [
        "### 2. Vertex ranking algorithm summarisation technique\n",
        "\n",
        "Using PyTextRank to find Phrases and Summarize text: Multi-word Phrase Extraction and Sentence Extraction for Summarization\n",
        "\n",
        "Inspired by the author of https://medium.com/@aneesha/beyond-bag-of-words-using-pytextrank-to-find-phrases-and-summarize-text-f736fa3773c5 \n",
        "(Notebook: https://github.com/DerwenAI/pytextrank/blob/master/example.ipynb)\n",
        "\n",
        "Another resource to take a look at: https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0by6FnE3w8n"
      },
      "source": [
        "betterNLP = BetterNLP() ### do not re-run this unless you wish to re-initialise the object"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w44pghUL3w8r"
      },
      "source": [
        "source_file='source.json'\n",
        "source_json_content='{\"id\":\"777\", \"text\":\"In an attempt to build an AI-ready workforce, SmartSoft Corp. announced Smart Colab Program which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Smart Colab Program will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Palo Alto giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and AI services such as SmartSoft Corp. Cognitive Services, Bot Services and Machine Learning Services. According to Mark Smith, Country AI Manager, SmartSoft Corp. India, said, ''With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That''s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.'' The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced SmartSoft Corp. Advanced Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\"}'\n",
        "f = open(source_file, 'w')\n",
        "f.write(\"%s\" % source_json_content)\n",
        "f.close()\n",
        "\n",
        "summarised_result = betterNLP.summarise(source_file, method=\"pytextrank\")\n",
        "\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
        "print(\"summarised_text=\",summarised_result['summarised_text'])\n",
        "print(\"token_ranks=\",summarised_result['token_ranks'])\n",
        "print(\"key_phrases=\",summarised_result['key_phrases'])\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "betterNLP.show_graph(summarised_result[\"graph\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87B-8mBQhQnf"
      },
      "source": [
        "!pip install --upgrade pytextrank>=2.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFaFdQu3hQng"
      },
      "source": [
        "!pip list | grep pytext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIJvzA3AhQng"
      },
      "source": [
        "import pytextrank\n",
        "import sys\n",
        "\n",
        "path_stage0 = \"dat/mih.json\"\n",
        "path_stage1 = \"o1.json\"\n",
        "\n",
        "with open(path_stage1, 'w') as f:\n",
        "    for graf in pytextrank.parse_doc(pytextrank.json_iter(path_stage0)):\n",
        "        f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n",
        "        # to view output in this notebook\n",
        "        print(pretty_print(graf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2kyTI6bfcz7"
      },
      "source": [
        "### 3. Build a simple text summarisation tool using NLTK\n",
        "\n",
        "Inspired by Wilame Lima Vallantin, the author of [Build a simple text summarisation tool using NLTK](https://medium.com/@wilamelima/build-a-simple-text-summarisation-tool-using-nltk-ff0984fedb4f).\n",
        "\n",
        "We have to break the text into sentences and tokens, remove stop-words. Tokenise words, calculate word frequency to determine if a word is important on the corpus, using the TF-IDF technique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOqeUNkg3w82"
      },
      "source": [
        "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf\")\n",
        "\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
        "print(\"summarised_text=\")\n",
        "pp.pprint(summarised_result['summarised_text'])\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r1Yw5i8hQni"
      },
      "source": [
        "print(\"important_words=\")\n",
        "pp.pprint(summarised_result['important_words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jQLb4Sqfc0E"
      },
      "source": [
        "### 4. Summarising text in python using a variation of TF-IDF method\n",
        "\n",
        "\n",
        "Inspired by Shivangi Sareen from the posts:\n",
        "[Summarise Text with TFIDF in Python 1](https://towardsdatascience.com/tfidf-for-piece-of-text-in-python-43feccaa74f8) and [Summarise Text with TFIDF in Python 2](https://medium.com/@shivangisareen/summarise-text-with-tfidf-in-python-bc7ca10d3284)\n",
        "\n",
        "We have to break the text into sentences and tokens, ***we do not remove stop-words*** but do remove special characters. Tokenise words, calculate word TF and IDF frequencies to determine if a word is important on the corpus, using the TF-IDF technique. And then based on the average score method filter out only those sentences that meet the criteria.\n",
        "\n",
        "We could also use the (average score + 1.5 * std dev) or (average score + 3 * std dev), depending on the size of the target documents to summarise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWAPOQyt3w87"
      },
      "source": [
        "summarised_result = betterNLP.summarise(generic_text, method=\"tfidf-ignore-stopwords\")\n",
        "\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "print(\"summarisation_processing_time_in_secs=\",summarised_result['summarisation_processing_time_in_secs'])\n",
        "pp.pprint(\"summarised_text=\" + summarised_result['summarised_text'])\n",
        "print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1Oov63jhQni"
      },
      "source": [
        "print(\"scored_documents=\")\n",
        "pp.pprint(summarised_result['scored_documents'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}